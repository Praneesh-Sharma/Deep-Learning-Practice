{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf79e5c",
   "metadata": {},
   "source": [
    "## Autograd : Automatic Differentiation Engine\n",
    "\n",
    "Automatic differentiation is a building block of every  deep learning library out there.\n",
    "\n",
    "PyTorch's Automatic Differentiation Engine called Autograd is a tool to understand how automatic differentiation works.\n",
    "Mordern neural architectures can have millions of learnable parameters. From a computation point of view, training a neural network consists of two phases.\n",
    "\n",
    "### Two phases of training NN(neural network)\n",
    "Forward propagation - makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
    "\n",
    "Backward propagation - Computes the gradients of the learnable parameters. The nueral network adjusts its parameters proportionate to the error and its guesses. It does this through three steps:\n",
    "    \n",
    "    1. Traverse back from the output\n",
    "    2. Collects derivatives of error with respect to the parameters of the functions/gradients\n",
    "    3. Optimizes the parameters using gradient descent\n",
    "    \n",
    "The forward propagation is pretty straghtforward. The output of one layer is the input to the next and so forth.\n",
    "\n",
    "Backward propagation is a bit more complicated since it requiresus to use the chain rule to compute the gradiesnts of the weights to the loss function. It is impractical to calculate gradients of such large composite functionsby solving mathematical equations, especially because these curves exist in a large number of dimensions and are impossible to fathom. This is where PyTorch's Autograd comes in.\n",
    "\n",
    "Autograd can calculate gradients of high-dimensionsal curves with only a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f191171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcee425d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02e15f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0177840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x000001BF6FB4FA90>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ee162e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f71d5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53b5a924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1f9a1b",
   "metadata": {},
   "source": [
    "## Advanced Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ff09a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0034, -0.0717, -0.5494], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ac0685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1027.5193,  -73.4579, -562.6027], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b154f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6218c174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e7b3e",
   "metadata": {},
   "source": [
    "### Frozen Parameters\n",
    "Parameters that dont compute gradients are called frozed parameters. It is useful to freeze part of your model if you know in advance that you wont need the gradients of those parameters. This offers some performace benefits by reducing AutoGrad computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26262dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e81476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da919260",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)  #pretrained resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "998ee0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze all the parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a0735",
   "metadata": {},
   "source": [
    "In ResNet, the last classifier is the last linear layer, model.fc . We can simply replace it with a new linear layer, unfrozen by default, that acts as our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "365912b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0762af01",
   "metadata": {},
   "source": [
    "All our parameters in this model, exceot for the  parameters of model.fc, are frozen. The only parameters that compute gradients are the weights and bias of model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32d002b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7df23d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
